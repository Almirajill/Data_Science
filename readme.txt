JEANNE MAY CAROLINO
ALMIRA JILL GARCIA
JOHN PAUL MONTER

Objectives (CAROLINO)

   -Describe what data science is and the role of data scientists.
      I. The data science
      II. The role of a data scientist
      III. The data science process
         A. Problem formulation
         B. Data collection
         C. Data preparation
         D. Exploratory data analysis and modeling
         E. Interpreting and communicating results

   -Difference in data and information
      I. Data in data science
      II. Information in data science
      III. Difference between information and data

   -Describe data processing life cycle
      I. What is data processing life cycle
      II. Stages of data processing life cycle

   -Understand different data types from diverse perspectives 
      I. What are the different data types

   -Describe data value chain in emerging era of big data. 
      I. Collection
      II. Publication
      III. Uptake
      IV. Impact

   -Understand the basics of Big Data. 

   -Describe the purpose of the Hadoop ecosystem components. 
      I. What is Hadoop Ecosystem
      II. Purpose of the key components of the Hadoop ecosystem





Introduction (GARCIA)

	In today's data-driven world, information is power. With the vast amounts of data available, data science has emerged as a critical field that helps organizations make informed decisions and gain a competitive edge. Data science is an interdisciplinary field that combines various techniques from computer science, statistics, mathematics, and domain expertise to extract valuable insights from data. These insights can be used to drive business decisions, optimize operations, and develop new products and services. In this way, data science has become a key enabler of digital transformation, with organizations across industries investing in data science to stay competitive in today's fast-paced and rapidly evolving market.
	
	Through this research, the aim is to understand the critical role that data science plays in modern society and contribute to the advancement of the field by identifying opportunities for further development. Also, this research aims to provide a comprehensive overview of data science by examining the role of data scientists, differentiating between data and information, describing the data processing life cycle, understanding various data types from diverse perspectives, discussing the data value chain in the emerging era of big data, and exploring the basics of big data and the purpose of Hadoop ecosystem components. 

	Over the past few decades, the world has experienced a data revolution, with the amount of data generated by individuals, organizations, and machines growing exponentially. Data science involves various techniques and fields that have gained significant attention in recent years due to the exponential growth of data and the advancements in computing technology. The field has emerged as a critical tool in various industries, including finance, healthcare, manufacturing, and technology. With the rise of Industry 4.0, data science has become even more relevant as it plays a critical role in unlocking the value of data generated by smart and connected systems. As a result, there is a growing demand for skilled data scientists who can help organizations harness the power of data to gain a competitive edge. However, as the field of data science continues to evolve, researchers are faced with various challenges such as data privacy concerns, interpretability issues, and bias, which need to be addressed to fully realize the potential of data-driven decision-making. Despite these challenges, data science has emerged as an essential tool for organizations across industries, shaping the future of how the world is understood and interacted with. Hence, there is a growing need for research that explores the current state of data science, the challenges it faces, and proposes potential solutions to overcome them and advance the field further. The insights and knowledge gained through data science have the potential to transform industries and society as a whole, making it crucial to continue exploring and advancing this field through ongoing research and innovation.





   Scope (GARCIA)

	By employing data science techniques, researchers can conduct enhanced data analysis, advanced modeling, and prediction. These methods enable them to uncover hidden patterns and trends within the data, providing deeper insights into the research problem. Additionally, data science facilitates text and image analysis, allowing researchers to extract information, perform sentiment analysis, and explore visual data. The field promotes reproducibility and transparency by documenting code and data preprocessing steps, fostering scientific collaboration. Furthermore, data visualization techniques aid in effectively communicating research findings, making complex data relationships and insights more accessible to readers. It empowers researchers to leverage the power of data, enabling rigorous analysis, robust modeling, and impactful conclusions.

PART B
Discussions 

   1. Describe what data science is and the role of data scientists.(GARCIA)

I. The data science

   The generation of vast amounts of digital data in our daily lives has made data science an increasingly critical field. Data science involves collecting and analyzing digital data, extracting insights, making informed decisions, and converting the data into meaningful and valuable actions. Additionally, data science is a multidisciplinary field that employs various tools, methods, and technologies that evolve over time. It combines probability, statistics, mathematics, data analysis, artificial intelligence, machine learning, computer science, algorithms, programming, and business to extract insights and knowledge from data. Therefore, businesses of all sizes require data science to study and transform data into practical information. 

   In addition, the types of data that data scientists analyze can be both structured and unstructured. Structured data in tabular format, organized in rows and columns and stored in a database, and unstructured data, including numbers, text, images, videos, or audio files, can be analyzed by data scientists. By analyzing these large volumes of structured and unstructured data, data scientists can produce meaningful insights and make informed decisions.

II. The role of a data scientist

   The application of data science in businesses is crucial, as it enables them to extract valuable insights and take necessary actions to remain competitive in the market. Data scientists are essential for companies to make better and more informed decisions. Data science enables businesses to identify new patterns and trends, establish relationships between variables, and visualize data. This can help companies maximize their profits, prevent significant losses, and predict and prevent future problems, including detecting and preventing fraud.

    Furthermore, data science can help companies gather customer feedback, develop new ideas for products and services, and create solutions to meet customers' needs. This, in turn, can result in greater customer satisfaction and increased profit. A data science strategy is also essential for businesses, as it allows them to attract new customers through targeted ads. Companies can use customers' browsing histories to gain insights into their interests and show them products and services that fit their preferences.

III. The data science process 

A. Problem formulation

   The initial phase of a data science project involves problem formulation, where the data scientist collaborates with stakeholders to identify the problem, determine the relevant data sources, and establish project goals. The key to this phase is to ask the right questions that will enable the team to gain a comprehensive understanding of the problem and define the appropriate questions that require answers. Questions such as "What happened?", "Why did it happen?", "What kind of data do we need to collect?", "What is the objective of the project?", "What are the current challenges?", "What can be done presently?", and "What will happen in the future?" may be useful during this phase. The purpose of this stage is to ensure that the problem is well defined and the right data is gathered to help address the issue effectively. This step is critical in the data science process, as it sets the stage for the rest of the project.

B. Data collection

   In the data science process, after the problem has been identified, relevant data must be collected from various sources. Data scientists play a significant role in this step, which involves identifying and collecting the appropriate data. To collect data, they first check the pre-existing data that is available to them. If that's insufficient, they move on to collecting new data from different sources. They collect data from various sources, such as internal company data, public data sets, relational databases, market research, surveys, web scraping, server logs, and website cookies. Data scientists must have access to large amounts of data to conduct their analysis, and they collect data from multiple sources using various methods. It's worth noting that the data collected at this stage is raw and could be corrupted, have missing values, or have errors.

C. Data preparation

   This is a crucial step in the data science process, involving the cleaning and transformation of raw data to make it usable. Raw data is typically disorganized and may contain missing values, outliers, and inconsistencies, making it necessary to clean and standardize the data before analysis. Data cleaning involves identifying missing data values and filling them in if necessary, correcting inaccuracies and errors, removing duplicate values, identifying corrupt records, and dealing with inconsistent data. This step is time-consuming, but it ensures the accuracy of the data for the rest of the data science process.

D. Exploratory data analysis and modeling

   After preparing the data, the next step in the data science process involves performing exploratory data analysis (EDA) to gain a better understanding of the data. Data exploration allows the analysis of the data in-depth, narrowing down the data that is essential for answering the initial questions, uncovering patterns, and extracting meaningful insights. Data scientists utilize statistical methods and data visualization tools to create diagrams, charts, and graphs that represent evident trends and correlations in the data. Based on the insights gained from the EDA, data scientists select an appropriate machine learning or statistical model to analyze the data. They use algorithms, machine learning, and artificial intelligence techniques to build, evaluate, deploy, and monitor a machine learning predictive model for the data. The model may be used for prediction, classification, or clustering, depending on the problem. Furthermore, data scientists perform hypothesis testing, predict and forecast highly accurate outcomes, and determine the best actionable steps for the future. This step is crucial in providing impactful recommendations based on the data analysis.

E. Interpreting and communicating results

   The final stage of the data science process involves communicating and presenting the results to stakeholders, clients, and decision-makers in an understandable and persuasive manner. It is crucial to present the findings in a way that non-technical individuals can comprehend. Effective communication skills are crucial for a data scientist to possess, as they play an essential role in conveying the significance and relevance of the insights and recommendations generated from the data. This phase is commonly referred to as "data storytelling," where the data scientist uses the results and insights obtained from the previous stages to weave a compelling narrative that highlights the relevance of the findings and their implications for the business. During this phase, the data scientist provides answers to the questions they identified in the first step of the process.





   2. Differentiate data and information.(GARCIA)

I. Data in data science

   In data science, data is a collection of raw and unorganized facts that require processing to make them meaningful. These individual units of information, represented by variables in analytical processes, can be considered as statistics and facts collected for reference or analysis. Data is meaningless until interpreted by either a human or a machine to derive meaning. It usually contains numbers, characters, and statements in a raw form.

II. Information in data science

   Information is defined as structured, processed, and organized data that is presented in context to make it relevant and useful to the intended recipient. Information is data that has been transformed and categorized into an understandable form that can be used in the decision-making process. Essentially, when data becomes useful after conversion, it is considered information. It provides answers to specific questions and can be obtained from a variety of sources, including newspapers, the internet, television, people, and books.

III. Difference in data and information

   Data:
    1. Data are the variables that help to develop ideas/conclusions.
    2. Data are text and numerical values.
    3. Data doesnâ€™t rely on information.
    4. Bits and bytes are the measuring units of data.
    5. Data can be easily structured as the following: 
         1. Tabular data 
         2. Graph 
         3. Data tree
    6. Data does not have any specific purpose.
    7. It is low-level knowledge.
    8. Data does not directly helps in decision making.
    9. Data is a collection of facts, which itself have no meaning.
    10. Example of data is student test score.

   Information:
    1. Information is meaningful data.
    2. Information is a refined form of actual data.
    3. While information relies on data.
    4. Information is measured in meaningful units like time, quantity, etc.
    5. Information can also be structured as the following: 
         1. Language 
         2. Ideas 
         3. Thoughts
    6. Information carries a meaning that has been assigned by interpreting data.
    7. It is the second level of knowledge.
    8. Information directly helps in decision making.
    9. Information puts those facts into context.
    10. Example of information is average score of class that is derived from given data.





   3. Describe data processing life cycle.(CAROLINO)
The data processing life cycle is a systematic approach to managing and transforming data from its raw state to valuable insights. It consists of several stages that work together to ensure that data is collected, prepared, analyzed, visualized, reported, and maintained accurately and effectively.
1) Data Collection is the first step involved in the data processing cycle. It is validated by prioritizing the information resources provided, such as the company's profits and losses, user data, staff data, market net worth, and other necessary data, after acquiring the raw information from other organizations in the enterprise.

2) Data Preparation in the processing cycle involves the duties to prioritize the data collected and filter out the unnecessary and inaccurate data. Before the conclusion, the errors in the data, replication of the data, duplicate data, and such misleading data are corrected and filtered to guarantee the quality. Once the data quality is checked, it is then encoded for further cycle. Because of all the methods used, the term "data cleaning" is also used to describe this step of data processing.

3) The Data Input stage of data processing is where the cleaned or encoded data is transformed into inputs that can be read by machines. It is ensured that the data is in a format that can be read by a computer and processed before feeding it to the computer. At this stage of the case, the priority is to validate the data before supplying it to the computer's Central Processing Unit (CPU).

4) Data Processing makes the data subject to various data processing methods like algorithmic and statistical calculations, depending on the tools that are being used. Most of the available tools make use of algorithms designed using machine learning and artificial intelligence for processing the cleaned data.

5) Data outputs are the form of data that has been processed and obtained by the previous steps. The output data obtained is then decoded which is ready to be displayed to the users. These outputs help the users immediately extract the statistics. Based on these extracted statistics, the presentation is done. For presentation, charts, reports, tables, graphs, and other such information-conveying statistics are used.

6) Storage is a major asset in any kind of data processing. The final step in the cycle is to store the decoded output data and metadata for future calibration and use. Through this, the user would be able to retrieve any particular data whenever needed. For a reliable use, the organizations buy huge amounts of memory to store all the data.
     In conclusion, the data processing life cycle is a continuous process that involves the collection, preparation, input, output, reporting, and storage. It is an iterative process, and the stages may be repeated multiple times to refine the results and improve the quality of the data. This process helps organizations to make informed decisions, gain a competitive advantage, and achieve their goals.





	4. Understand different data types from diverse perspectives.(CAROLINO)

Data can be classified into different types based on the way it is collected, its structure, and its content. Understanding the different data types is important because it helps in selecting the appropriate analytical methods and tools to extract insights from the data.
Numerical Data:
Statistical Perspective: Numerical data can be analyzed using statistical techniques such as measures of central tendency, dispersion, correlation, and regression.
Scientific Perspective: Numerical data can be used to represent measurements, observations, and experimental results in various scientific fields.
Business Perspective: Numerical data can be employed for financial analysis, forecasting, sales figures, and other quantitative business metrics.


Categorical Data:
Sociological Perspective: Categorical data can provide insights into social divisions, demographics, cultural attributes, and group affiliations.
Marketing Perspective: Categorical data can assist in market segmentation, customer profiling, target audience identification, and product preferences.
Political Perspective: Categorical data can be utilized to analyze voting patterns, political affiliations, public opinion, and demographic influences on elections.
 
Textual Data:
Natural Language Processing Perspective: Textual data can be processed using NLP techniques like sentiment analysis, topic modeling, named entity recognition, and text classification.
Information Retrieval Perspective: Textual data can be indexed and searched to extract relevant information, build search engines, and enable document clustering.
Customer Support Perspective: Textual data can be analyzed to understand customer feedback, sentiment, and trends, helping improve products and services.

Geospatial Data:
Environmental Perspective: Geospatial data can be used to analyze ecosystems, land use patterns, climate change, and natural resource management.
Urban Planning Perspective: Geospatial data can aid in city planning, infrastructure development, traffic management, and location-based services.
Disaster Management Perspective: Geospatial data can support emergency response, risk assessment, evacuation planning, and spatial modeling of hazards.

Time Series Data:
Financial Perspective: Time series data can be utilized for stock market analysis, asset pricing, forecasting, and risk management.
Medical Perspective: Time series data can help monitor patient health, analyze physiological signals, detect anomalies, and predict disease progression.
Energy Perspective: Time series data can assist in energy consumption analysis, demand forecasting, grid optimization, and renewable energy planning.

In conclusion, understanding the different data types and their characteristics is essential for data scientists to select the appropriate methods and tools to analyze and interpret the data. Each type of data requires a specific approach to analysis, and the analytical techniques used must be adapted to the structure and content of the data to extract valuable insights.



   
   7. Describe the purpose of the Hadoop ecosystem components.(CAROLINO)

	The Hadoop ecosystem is a collection of open-source software components that work together to enable the storage, processing, and analysis of large-scale data sets. Each component within the ecosystem has a specific purpose and plays a crucial role in building a distributed and scalable data processing infrastructure. Here are some key components of the Hadoop ecosystem and their purposes:

Hadoop Distributed File System (HDFS): HDFS is a distributed file system that provides reliable, scalable, and fault-tolerant storage for big data. It breaks large files into blocks and distributes them across a cluster of machines, allowing for high throughput and parallel processing.
MapReduce: MapReduce is a programming model and processing framework for distributed data processing. It allows developers to write parallelizable algorithms that can process large amounts of data by dividing the workload into maps and reducing tasks, which are executed in parallel across the cluster.
YARN (Yet Another Resource Negotiator): YARN is a resource management framework in Hadoop that handles resource allocation and scheduling of tasks. It manages the cluster resources and enables different data processing frameworks, such as MapReduce, Apache Spark, and Apache Flink, to run concurrently on the same Hadoop cluster.
Apache Hive: Hive is a data warehouse infrastructure built on top of Hadoop that provides a high-level query language, HiveQL, similar to SQL. It allows users to write queries and perform data analysis on large datasets using a familiar SQL-like interface.
Apache Pig: Pig is a high-level data flow scripting language and runtime environment for Hadoop. It allows users to express data transformations and analysis tasks using a scripting language called Pig Latin. Pig optimizes and executes these scripts on the Hadoop cluster.
Apache Spark: Spark is a fast and general-purpose distributed data processing engine that can run on top of Hadoop. It provides an in-memory computing capability and supports various programming languages, including Scala, Java, Python, and R. Spark offers a rich set of libraries for batch processing, stream processing, machine learning, and graph processing.
Apache HBase: HBase is a distributed, scalable, and column-oriented NoSQL database that provides real-time read/write access to large datasets. It is built on top of HDFS and is designed for random, low-latency access to big data, making it suitable for applications that require fast data retrieval.
Apache Kafka: Kafka is a distributed streaming platform that allows for the storage and real-time processing of high-throughput, fault-tolerant, and event-driven data streams. It provides pub-sub messaging capabilities and is often used for building real-time data pipelines and streaming applications.

These are just a few examples of the components within the Hadoop ecosystem. There are several other tools and frameworks available, such as Apache ZooKeeper for distributed coordination, Apache Flume for collecting and aggregating log data, and Apache Sqoop for data integration between Hadoop and relational databases. The purpose of the Hadoop ecosystem components is to provide a comprehensive set of tools and technologies for handling big data challenges, including storage, processing, analysis, and streaming, in a distributed and scalable manner.

Conclusion (MONTER) 





Recommendation (CAROLINO)

	For future researchers in data science, there are numerous exciting avenues to explore. One promising area is the development of advanced machine learning algorithms with a focus on interpretability. By devising techniques to understand and explain the decisions made by complex models, researchers can enhance transparency, fairness, and trust in AI systems. Additionally, investigating fairness and bias in machine learning is crucial for addressing the ethical implications of algorithmic decision-making. Researchers can develop methods to mitigate bias, ensure fairness, and promote equitable outcomes in various domains. Another area of interest is the advancement of natural language processing (NLP) techniques. Researchers can delve into tasks like sentiment analysis, text summarization, and language translation, leveraging recent breakthroughs in deep learning and transformer models. Furthermore, exploring time series analysis and forecasting methods provides opportunities to develop models capable of capturing temporal dependencies, handling missing data, and making accurate predictions. Other exciting research areas include causal inference, reinforcement learning, privacy-preserving techniques, robustness and security of machine learning systems, data visualization, and human-computer interaction. By focusing on these and related topics, future researchers can contribute to the growth and practical applications of data science while addressing real-world challenges.