JEANNE MAY CAROLINO
ALMIRA JILL GARCIA
JOHN PAUL MONTER

PART A
   Objectives (CAROLINO)
   Introduction (GARCIA)
   Summary/Conclusion (MONTER)
	In conclusion, data science is crucial for assisting organizations in making wise decisions and gaining a competitive edge by deriving insightful knowledge from unstructured data. Data scientists gather, clean, analyze, and interpret data using various tools and procedures to turn it into useful information. For practical data analysis and decision-making, one must be able to distinguish between data and information. Data science promotes digital transformation by utilizing the power of data to progress many sectors.

	The data processing life cycle offers a systematic method for handling and turning data into insightful information. It entails collecting, preparing, analyzing, visualizing, reporting, and maintaining data. Different data kinds, including numeric, category, text, time series, spatial, and binary data, necessitate distinct analysis methods. Data scientists may get valuable insights and influence choices by identifying and taking advantage of these various data kinds.

	Organizations may optimize the value produced by data at each step of its lifespan using the data value chain framework. It is essential to emphasize the significance of data gathering, publication, uptake, and impact. By adhering to this paradigm, organizations may ensure efficient data usage, promote sharing and reuse, and promote good changes and results.

	Big data presents difficulties because of its size, speed, diversity, truthfulness, and worth. To extract worthwhile insights, complex data processing techniques are needed. Organizations must deal with issues including data management, information overload, data security, and rapidly changing technology. Data heroes, including data scientists, engineers, and officers, are essential to efficiently utilizing big data. Organizations may unleash the potential of big data by adopting a data-driven strategy and using the appropriate tools and processes.

	The Hadoop ecosystem is a powerful collection of open-source software tools that enables handling and storing enormous amounts of data. Various components, including HDFS, MapReduce, YARN, Hive, Pig, Spark, HBase, and ZooKeeper, provide scalability, fault tolerance, and practical data analysis. By providing an affordable and effective method for managing and analyzing enormous datasets, the Hadoop ecosystem revolutionizes the management and analysis of big data.


PART B
Discussions 
   1. Describe what data science is and the role of data scientists.(GARCIA)
   2. Differentiate data and information.(GARCIA)
   3. Describe data processing life cycle.(CAROLINO)
   4. Understand different data types from diverse perspectives.(CAROLINO)
   5. Describe data value chain in emerging era of big data.(MONTER)
	Let us first grasp the value chain concept to understand the data value chain. A value chain represents a business's sequential steps to create and deliver a product or service to its customers. It encompasses everything from ideation and production to delivery and post-consumption considerations. It is a vital framework for business planning and organization to succeed. In the 1980s, Porter proposed the value chain concept.
	Beyond its application in business, the value chain can also be utilized as an analytical tool to evaluate information flow and comprehend the value generated by data technology. By examining the different stages involved in data collection, processing, and utilization, we gain insights into how data technology generates value and enhances various aspects of our lives. This understanding emphasizes the importance of each step within the data value chain and its contribution to the overall impact of data technology.
	It has been common practice to overlook the value of data sharing and usage in favor of overly emphasizing data collection and generation. Data sharing has only sometimes been widespread among some people or groups. It is crucial to realize that knowledge is a precious resource that may be utilized and recycled for little to no additional expense. Our goal should be to optimize data usage and reuse to maximize its value. Simply gathering and distributing data does not guarantee its utilization or positive impact. It is crucial to pay greater attention to how data is collected, shared, and utilized, and the data value chain serves as a tool to understand and visualize this process.
	Let us delve into the Data Value Chain (DVC), a systematic framework that outlines a series of procedures to extract maximum value from data at each stage of its lifespan. Like other value chains, the DVC ensures effective data utilization by repeating these steps consistently throughout its entire journey. While traditional value chains focus on managing systems, inputs, and outputs to optimize the end product's quality, cost, and profit, the data value chain differentiates itself by primarily aiming to generate actionable insights rather than tangible products or services. 
	The data value chain serves multiple purposes. As a teaching tool, data can be used to demonstrate the complex processes involved in data generation, usage, and influence. Additionally, it can function as a management tool to monitor and evaluate the data production process, ensuring its effectiveness and efficiency.
	The data value chain comprises four categories: collection, publication, uptake, and impact. In addition to these categories, the data value chain consists of twelve phases. These phases include identifying the data need, collecting and processing it, analyzing it, and subsequently releasing, disseminating, connecting, incentivizing, influencing, and utilizing it for change and impact. Constant feedback between data producers and stakeholders is crucial throughout the process.

Collection: This stage highlights the most basic yet critical statistical activities that focus on determining what data type is needed to solve a problem, answer a question, or monitor a process.

	Identify - recognize the need for data and specify the data requirements to address a particular problem or objective.

	Collect - this process involves data collection and gathering from various sources, such as surveys or retrieval of administrative data or the use of remote sensing methods.

	Processing - includes ensuring that data is appropriately recorded, categorized, and stored in formats. It performs tasks such as cleaning, organizing, converting, and organizing data to make it available for analysis.


Publication: Following data collection, it is necessary to publish both the metadata and the data itself in a manner that allows users of the data to access them. It includes making sure that the data stored is in an open and accessible format, as well as providing adequate documentation to ensure users can understand and interpret the data.

	Analyze - examine the obtained data using various statistical, computational, and analytical approaches to gain insights, detect trends, or make predictions.

	Release - the findings and results of the analysis are compiled and documented into reports, publications, or presentations that can be accessed online or offline that contain machine-readable data.

	Disseminate - the act of providing stakeholders, such as decision-makers or the general public, with the studied data and its insights. This procedure entails publishing recorded reports, publications, or presenting the data.

Uptake: There are several ways in which users can be connected: through press releases and online dissemination; by holding training, seminars, or other educational events; and by improving the user experience on websites, data portals, and archives. Incentives may take a variety of forms.

	Connect - once the data has been collected and published, it is essential to utilize current technology to encourage users to provide more data. This data will enable the reprocessing of the collected data, leading to the discovery of new insights and trends. This data loop is critical for keeping data-gathering procedures accurate and up to date. Additionally, it enables the creation of fresh approaches that will enhance the data-collecting procedure and yield more insightful outcomes.

	Incentivize - In this process, we can make the data access and analysis tools more straightforward and user-friendly, reducing the time and cost associated with using the data. This process will help analyze new data by automating repetitive tasks and keeping this new data up-to-date. To reinforce the value of data utilization, showcasing successful examples of how data-driven decisions have positively influenced outcomes is crucial. Demonstrating the practical benefits, users will understand the value that data can bring to their decision-making processes.

	Influence - this is a process through which users learn and value the worth of data. This procedure involves establishing a culture that encourages the use of data in decision-making processes. In order to accomplish this, fostering an environment in which individuals are provided with the opportunity to use data and emphasizing the benefits and advantages of data-driven decisions, such as greater accuracy, informed insights, and better decision-making. 

Impact: It is common to view the use of development data as the end goal. However, usage occurs during various stages between the publication of the data and encouraging a decision-maker to modify decisions as a result of the data. In order to encourage support for data production, the general public should see apparent positive changes as a result of using the data. Additionally, data must be available, with free licensing for both use and reuse, while providing supplementary resources and capacity-building opportunities. Once this process is in place, the channels connecting the data and its users will strengthen, ensuring a more entrenched and influential data impact.
	
	Use - This stage involves utilizing the data to gain a deeper understanding of a problem or to make informed decisions. It serves as the initial end-use of the data, which is implemented to address specific challenges or support decision-making processes. This iterative process of using data, receiving feedback, which can provide valuable insights, and making adjustments help continuously improve the impact and effectiveness of data utilization.

	Change - this process is vital in bringing about meaningful changes and improvements. It will use the data to utilize in order to bring positive outcomes in projects or to enhance existing situations. Monitoring the behavioral changes can gain insights into the effectiveness of their data-driven approach. That will allow them to identify the patterns and trends that highlight the impact of data utilization on decision-making and actions. Helping formulate data-driven policies that identify the relationship between data utilization and positive outcomes that leverage data effectively.

	Reuse - entails utilizing current data and maximizing its value by merging it with additional relevant data sources. This process enables the development of comprehensive and meaningful analyses that extend beyond the constraints of particular datasets. Data must also be freely shared and made available to various people to have the most significant impact. This promotes teamwork, sharing of data, and idea exchange. Furthermore, encouraging the development of data-use skills and competencies is critical for successfully motivating individuals and organizations to use data for decision-making and problem-solving. By boosting interest in data generation and showcasing the value and advantages that data can provide, a culture of data-driven practices may be nurtured, leading to improve informed decision-making and beneficial outcomes.



   6. Understand the basics of Big Data.(MONTER)

	Big data is a broad and intricate collection of information that is complex to manage with conventional methods. It is distinguished by its size, speed, and complexity, which makes it challenging to manage with standard data processing methods. Conventional data management solutions are incapable of successfully storing and analyzing such enormous volumes of data. Big data also includes all of the many types of structured, semi-structured, and unstructured data that businesses gather. This data has enormous potential for extracting significant insights and driving meaningful results using advanced data analysis approaches such as predictive modeling and machine learning.

Types of Big Data

	Big data repositories hold a variety of information types and they may typically be divided into three widely used categories:

	i. Structured data - refers to information stored in a well-organized and fixed format. It has a clear structure that both computers and humans can understand. This data may be conveniently stored in databases and retrieved using straightforward approaches. Examples of structured data include tables and spreadsheets businesses typically maintain in their databases.

	ii. Unstructured data - is data that lacks a recognized structure. It is characterized by its extensive size and heterogeneity, surpassing that of structured data. Unstructured data does not conform to any predefined organization or clear format. This type of data presents challenges in handling, comprehending, and analyzing. Examples of unstructured data include social media comments, tweets, shares, and posts, as well as the videos users view on platforms like YouTube.

	iii.Semi-structured data - combines structured and unstructured data. Semi-structured data does not adhere to a predefined database format, but it does contain crucial tags or labels that aid in the differentiation of different elements within the data. Even if the data does not fit neatly into a specified framework, these tags give some degree of order and allow for some categorization. Semi-structured data includes XML files, JSON documents, and log files, which include some organization or labeling but less than completely organized data.


Characteristics of Big Data

	Big data refers to large amounts of information businesses collect and store using unique systems. Comprehending big data's fundamental components is crucial to make sense of it. Here are the main characteristics of big data:

	1. Volume: The volume of data refers to how much there is - measured in units like gigabytes, zettabytes, and yottabytes. This characteristic includes various data types, such as transactions, IoT device data, videos, images, audio, and social media content. The volume of data helps determine if it qualifies as big data.

	2. Velocity: Velocity refers to the speed of generation of data. It is about how quickly data is produced and processed to meet demands. It includes the continuous flow of data from sensors, social media platforms, and application logs, which generate large volumes of data.

	3. Variety: Data that is available in a variety of formats, such as structured, unstructured, and semi-structured data. Effective data management involves organizing and handling various data types from multiple sources.
	
	4. Veracity: Veracity relates to the quality and reliability of data. Data from diverse sources is essential to ensure data accuracy, consistency and eliminate irrelevant information. Data veracity involves linking, matching, cleaning, and transforming data across different systems.

	5. Value: Value refers to the benefits and insights that data can provide to an organization. It is the most important aspect of big data, as it represents the usefulness and potential of data to drive decision-making and generate new predictions.
	

	
	The phrase "big data" became famous in 2001 when Doug Laney identified its essential qualities as the "3 Vs of Data": volume, velocity, and variety. Big data has grown rapidly as a result of the World Wide Web's creation in 1989 and the introduction of Hadoop, an open-source software framework for large-scale data storage, in 2005. Since the advent of the Internet of Things (IoT) and machine learning, data collection has multiplied tremendously.

	Cloud computing has been critical in enhancing the capabilities of big data by providing scalable storage and processing resources. Graph databases have also shown to be valuable tools for evaluating large datasets. Despite advancements, the actual potential of big data is still being explored, with companies increasingly relying on advanced analytics and predictive models to glean insights from massive volumes of data.



	Big data is a valuable resource that offers professionals a variety of information that can be used to improve their company. Here are some of the main reasons why big data is important:

	- Cost savings: Big data systems such as Apache, Hadoop, and Spark assist enterprises in saving money by identifying more effective operating techniques. Companies may cut expenditures and prevent losses by employing analytics to anticipate the possibility of product returns.

	- Increased efficiency: Big data technologies enable firms to collect and analyze data from various sources. This enables better decision-making, automation of repetitive operations, and increased operational performance.

	- Market analysis: Big data analysis assists businesses in better understanding the market. Businesses may find popular things and surpass competitors by examining buying trends. Big data improves accuracy and insight for supplier networks and B2B (Business-to-business) communities.

	- Improved client experiences: Organizations may adjust items to their target market by examining consumer habits. This leads to customized marketing strategies, fulfilling customer expectations, and enhanced brand loyalty.

	- Innovation support: Big data analytics reveals insights that promote company innovation. It aids in developing new goods and services, updating current ones, and the maintenance of competitiveness by monitoring feedback, product success, and rival companies.

	- Fraud detection: Financial institutions and the government utilize big data to detect fraud. Artificial intelligence and machine learning algorithms examine transaction patterns to discover anomalies and probable fraud.

	- Increased productivity: Big data solutions allow data scientists and analysts to efficiently review enormous volumes of data, raising production levels and enhancing data pipeline efficiency.

	- Agility: Big data analytics aids in the innovation and adaptability of enterprises. By evaluating consumer data, businesses may stay ahead of the competition and efficiently address customer problem areas. It also allows for risk assessment, product/service improvement, and informed decision-making.

	Data collection and use are not the only factors that determine the significance of big data. By evaluating data from multiple sources, businesses may simplify resource management, enhance efficiency, optimize product development, drive revenue growth, and make educated decisions. When paired with high-performance analytics, big data enables near-real-time issue identification, quick medical picture analysis, portfolio recalculations, accurate fraud detection, and other capabilities.


	Big data must be understood in its movement across many locations, sources, systems, owners, and users before it can be used by enterprises efficiently. There are five critical stages to taking control of this "big data fabric," which includes both structured and unstructured data:

	1. Create a big data strategy and a plan for collecting, managing, storing, sharing, and using data inside and outside the organization. Align the organizational objectives with technology objectives using big data as a significant business asset.

	2. Recognize large data sources such as streaming data from Internet of Things (IoT) devices, social media interactions, publicly available data, and other sources such as data lakes, cloud data, suppliers, and consumers.

	3. Utilize modern computer tools to quickly access and handle enormous amounts and varieties of big data. Manage and store extensive data. Put quality control, governance, and storage techniques into practice. Consider adaptable choices like cloud computing, data lakes, pipelines, and Hadoop for storing and handling large data.

	4. Data analysis: Perform thorough studies on all accessible big data using high-performance technologies like grid computing or in-memory analytics, or choose pertinent data in advance for study. Machine learning and artificial intelligence (AI) are two elements of big data analytics that can provide successful business outcomes.

	5. Utilize a data-driven approach to decision-making by relying on well-managed and reliable data. Instead of relying solely on intuition, base judgments on the evidence offered by big data. Organizations that are driven by data operate more profitably, efficiently, and effectively.


	In spite of the numerous benefits that big data can offer businesses, it also comes with several challenges. Considering that vast quantities of personal information are available, companies must prioritize data security to ensure that the data they collect and process stays private, especially with the recent data breaches. Furthermore, big data can also invite more information overload and noise, reducing the usefulness of those data sets. Thus, as a result of the increase in massive volumes of data, companies now have to be able to distinguish between relevant signals and irrelevant ones in order to operate efficiently.

	In addition, managing the sheer amount of data that organizations have to deal with is a struggle regardless of the advances in storage technology. It remains a constant challenge for data managers to find efficient and effective ways to manage and store data. Moreover, data scientists need to clean and organize their data to ensure that they can be analyzed meaningfully and that the data is relevant, some of which can be time-consuming. Finally, the rapidly changing landscape of big data technology adds to the challenges of implementing such technologies.


	The era of big data has transformed how organizations use and extract meaningful insights from massive volumes of data. It has transformed the field of business intelligence. 
	
Accept data-driven decision-making.
	- Various sources, including automobiles, wearables, and appliances, create big data. To fully realize its potential, enterprises must take a data-driven strategy. Data gathering, data analysis, and data use constitute three pillars of building a solid foundation. Businesses may overcome hurdles and realize the full potential of data-driven decision-making by using big data solutions and following best practices.

The Role of Data Heroes
	- Data heroes are the driving force behind efficiently using big data. Data scientists are critical in evaluating data and gaining new insights. Data engineers use DataOps concepts to construct efficient data pipelines. Data officers ensure data reliability and accountability. Collaboration and synergy between these positions are critical for the success of analytics efforts. Organizations may maximize their data-driven initiatives by understanding the contributions of these data heroes.

The Data Lake vs. the Data Warehouse
	- The words "data lake" and "data warehouse" are frequently used interchangeably, which leads to misunderstanding. They do, however, fulfill diverse functions. A data lake is a depository for large volumes of raw and unprocessed data. It allows for greater flexibility and agility in data investigation. On the other hand, a data warehouse is a structured storage system that stores processed and organized data for simple analysis and reporting. Understanding the distinctions between these two notions is critical for selecting the best method depending on individual company requirements.

Big Data and Cloud Computing
	- Big data initiatives need a substantial amount of processing power and storage space. By using its scalability and flexibility, cloud computing provides a cost-effective alternative. Businesses may efficiently manage massive amounts of data by merging big data technology with cloud infrastructure. This combination enables enterprises to achieve data processing and storage agility, improving performance and operational efficiency.

AI and ML
	- Artificial intelligence (AI) and machine learning (ML) have emerged as game changers in big data technologies. These advanced technologies enable computers to do astonishing feats such as picture recognition, speech transcription, text reading, and even comprehending the mood behind social media replies. AI and ML, when joined with data lakehouses, usher in a new era of possibilities by gathering various data kinds and comprehending and utilizing that data to trigger activities and assist decision-making processes.
	Incorporating AI and ML into data lakehouses has cleared the door for various previously imagined innovative application cases. The purpose of data lakehouses was originally to store enormous volumes of data. However, with the addition of AI and ML, these data lakehouses can now do more than just store data. Comprehending stored data enables them to trigger actions or provide valuable insights to make informed decisions.





   7. Describe the purpose of the Hadoop ecosystem components.(CAROLINO)
